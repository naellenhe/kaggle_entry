{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot  # plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from csv files into Pandas DataFrames\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785) train: first column contains true label, 28x28 image\n",
      "(28000, 784) 28x28 image\n"
     ]
    }
   ],
   "source": [
    "# See what the data looks like\n",
    "print(train.shape, 'train: first column contains true label, 28x28 image') \n",
    "print(test.shape, '28x28 image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:, 1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "train.iloc[:, 1:] = train.iloc[:, 1:].apply(lambda x: x / 255)\n",
    "test = test.apply(lambda x: x / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel149</th>\n",
       "      <th>pixel150</th>\n",
       "      <th>pixel151</th>\n",
       "      <th>pixel152</th>\n",
       "      <th>pixel153</th>\n",
       "      <th>pixel154</th>\n",
       "      <th>pixel155</th>\n",
       "      <th>pixel156</th>\n",
       "      <th>pixel157</th>\n",
       "      <th>pixel158</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41990</th>\n",
       "      <td>0.14902</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>0.631373</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.631373</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41991</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.513725</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel149  pixel150  pixel151  pixel152  pixel153  pixel154  pixel155  \\\n",
       "41990   0.14902  0.439216  0.631373  0.866667  0.996078  0.960784  0.631373   \n",
       "41991   0.00000  0.000000  0.000000  0.000000  0.513725  0.996078  0.996078   \n",
       "\n",
       "       pixel156  pixel157  pixel158  \n",
       "41990  0.200000  0.000000       0.0  \n",
       "41991  0.823529  0.011765       0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[41990:41992,150:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this digit is 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x116f5d358>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"this digit is\", train.iloc[25,0])\n",
    "digit = train.iloc[25, 1:785].values.reshape(28,28)\n",
    "pyplot.imshow(digit, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get smaller subset\n",
    "num_sample = 42000\n",
    "\n",
    "train_small = train.iloc[:(num_sample-1000), :]\n",
    "validation_small = train.iloc[(num_sample-1000):num_sample, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41000, 785)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 785)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image = train_small.iloc[:, 1:].values.reshape(-1, 1, 28, 28)\n",
    "train_label = train_small.iloc[:, 0].values\n",
    "print(train_image.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_image = torch.FloatTensor(train_image)\n",
    "t_label = torch.LongTensor(train_label)\n",
    "train_dataset = list(zip(t_image, t_label))\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 28, 28)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "validation_image = validation_small.iloc[:, 1:].values.reshape(-1, 1, 28, 28)\n",
    "validation_label = validation_small.iloc[:, 0].values\n",
    "print(validation_image.shape)\n",
    "print(validation_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_image = torch.FloatTensor(validation_image)\n",
    "v_label = torch.LongTensor(validation_label)\n",
    "validation_dataset = list(zip(v_image, v_label))\n",
    "len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = test.values.reshape(-1, 1, 28, 28)\n",
    "test_dataset = torch.FloatTensor(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=len(test_dataset), \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: torch.Size([100, 1, 28, 28]) \n",
      "labels: torch.Size([100])\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.1373,  0.9922,  0.9922,  0.6275,\n",
      "         0.1569,  0.3882,  0.9922,  0.9922,  0.5882,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print('images:', images.shape, '\\nlabels:', labels.shape)\n",
    "    print(images[1][0][6])\n",
    "    print(labels[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: torch.Size([100, 1, 28, 28])\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.2353,  0.8941,  0.9922,  0.9922,  0.9922,\n",
      "         0.9922,  0.9922,  0.9922,  0.9922,  0.8118,  0.7725,  0.1804,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "for images in test_loader:\n",
    "    print('images:', images.shape)\n",
    "    print(images[0][0][6])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchDeepConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(PytorchDeepConvNet, self).__init__()\n",
    "\n",
    "        # Layer 1: conv - relu - conv- relu - pool\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))        \n",
    "        \n",
    "        # Layer 2: conv - relu - conv- relu - pool\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        # Fully Connected 1 (readout)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, num_classes)\n",
    "        \n",
    "        # Initialize all parameters using kaiming normalization\n",
    "        self.init_weights_kaiming()\n",
    "    \n",
    "    def init_weights_kaiming(self):\n",
    "        for layer in [self.layer1, self.layer2]:\n",
    "            for m in layer:\n",
    "                if type(m) == nn.Conv2d:\n",
    "                    m.weight = nn.init.kaiming_normal_(m.weight)\n",
    "        \n",
    "        self.fc1.weight = nn.init.kaiming_normal_(self.fc1.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "    \n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PytorchDeepConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = PytorchDeepConvNet(num_classes)\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Use Adam as the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50. Loss: 0.2688966393470764.\n",
      "Iteration: 100. Loss: 0.19573372602462769.\n",
      "Iteration: 150. Loss: 0.13630276918411255.\n",
      "Iteration: 200. Loss: 0.06110623851418495.\n",
      "Iteration: 250. Loss: 0.11940505355596542.\n",
      "Iteration: 300. Loss: 0.08742796629667282.\n",
      "Iteration: 350. Loss: 0.0591735802590847.\n",
      "Iteration: 400. Loss: 0.023604927584528923.\n",
      "Iteration: 450. Loss: 0.036535002291202545.\n",
      "Iteration: 500. Loss: 0.03368818014860153.\n",
      "Iteration: 550. Loss: 0.0860200896859169.\n",
      "Iteration: 600. Loss: 0.03653855249285698.\n",
      "Iteration: 650. Loss: 0.12522092461585999.\n",
      "Iteration: 700. Loss: 0.01993611454963684.\n",
      "Iteration: 750. Loss: 0.04363555461168289.\n",
      "Iteration: 800. Loss: 0.07630090415477753.\n",
      "Iteration: 850. Loss: 0.026916280388832092.\n",
      "Iteration: 900. Loss: 0.032773490995168686.\n",
      "Iteration: 950. Loss: 0.1278548240661621.\n",
      "Iteration: 1000. Loss: 0.029393989592790604.\n",
      "Iteration: 1050. Loss: 0.04361877962946892.\n",
      "Iteration: 1100. Loss: 0.010259303264319897.\n",
      "Iteration: 1150. Loss: 0.08307251334190369.\n",
      "Iteration: 1200. Loss: 0.08247904479503632.\n",
      "Iteration: 1250. Loss: 0.00751527538523078.\n",
      "Iteration: 1300. Loss: 0.03849942609667778.\n",
      "Iteration: 1350. Loss: 0.00642409548163414.\n",
      "Iteration: 1400. Loss: 0.05884092301130295.\n",
      "Iteration: 1450. Loss: 0.011619504541158676.\n",
      "Iteration: 1500. Loss: 0.0481145903468132.\n",
      "Iteration: 1550. Loss: 0.017070407047867775.\n",
      "Iteration: 1600. Loss: 0.007984640076756477.\n",
      "Iteration: 1650. Loss: 0.02559482492506504.\n",
      "Iteration: 1700. Loss: 0.00620271498337388.\n",
      "Iteration: 1750. Loss: 0.013637282885611057.\n",
      "Iteration: 1800. Loss: 0.03657940402626991.\n",
      "Iteration: 1850. Loss: 0.008708307519555092.\n",
      "Iteration: 1900. Loss: 0.0021859346888959408.\n",
      "Iteration: 1950. Loss: 0.012246852740645409.\n",
      "Iteration: 2000. Loss: 0.007793467491865158.\n",
      "Iteration: 2050. Loss: 0.015151623636484146.\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "# accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images) # Now we dont need to resize like images.view(xx)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: Softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t paramters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 50 == 0:\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}.'.format(iter, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 validation images: 98.6 %\n"
     ]
    }
   ],
   "source": [
    "wrong_predictions = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in validation_loader:\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # See which are error predictions\n",
    "        result = (predicted == labels)\n",
    "        err_imgs = images[result == 0] # 0 means wrong prediction\n",
    "        err_labels = labels[result == 0]\n",
    "        err_outputs = predicted[result == 0]\n",
    "        for img, lbl, out in zip(err_imgs, err_labels, err_outputs):\n",
    "            wrong_predictions.append((img, lbl, out))\n",
    "     \n",
    "    print('Test Accuracy of the model on the 1000 validation images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images in test_loader:\n",
    "        images = Variable(images)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12aa18278>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(images[3].reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28000])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId  Label\n",
       "0        1      2\n",
       "1        2      0\n",
       "2        3      9\n",
       "3        4      0\n",
       "4        5      3"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame({'Label': predicted.numpy(), 'ImageId': list(range(1, predicted.shape[0] + 1))})\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('prediction.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
