{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot  # plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from csv files into Pandas DataFrames\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "test = pd.read_csv('dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 785) train: first column contains true label, 28x28 image\n",
      "(28000, 784) 28x28 image\n"
     ]
    }
   ],
   "source": [
    "# See what the data looks like\n",
    "print(train.shape, 'train: first column contains true label, 28x28 image') \n",
    "print(test.shape, '28x28 image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[:, 1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "train.iloc[:, 1:] = train.iloc[:, 1:].apply(lambda x: x / 255)\n",
    "test = test.apply(lambda x: x / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel149</th>\n",
       "      <th>pixel150</th>\n",
       "      <th>pixel151</th>\n",
       "      <th>pixel152</th>\n",
       "      <th>pixel153</th>\n",
       "      <th>pixel154</th>\n",
       "      <th>pixel155</th>\n",
       "      <th>pixel156</th>\n",
       "      <th>pixel157</th>\n",
       "      <th>pixel158</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41990</th>\n",
       "      <td>0.14902</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>0.631373</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.631373</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41991</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.513725</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>0.996078</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel149  pixel150  pixel151  pixel152  pixel153  pixel154  pixel155  \\\n",
       "41990   0.14902  0.439216  0.631373  0.866667  0.996078  0.960784  0.631373   \n",
       "41991   0.00000  0.000000  0.000000  0.000000  0.513725  0.996078  0.996078   \n",
       "\n",
       "       pixel156  pixel157  pixel158  \n",
       "41990  0.200000  0.000000       0.0  \n",
       "41991  0.823529  0.011765       0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.iloc[41990:41992,150:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this digit is 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11b59c320>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"this digit is\", train.iloc[25,0])\n",
    "digit = train.iloc[25, 1:785].values.reshape(28,28)\n",
    "pyplot.imshow(digit, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get smaller subset\n",
    "num_sample = 42000\n",
    "\n",
    "train_small = train.iloc[:(num_sample-1000), :]\n",
    "validation_small = train.iloc[(num_sample-1000):num_sample, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41000, 785)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 785)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 5 #20\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41000, 1, 28, 28)\n",
      "(41000,)\n"
     ]
    }
   ],
   "source": [
    "train_image = train_small.iloc[:, 1:].values.reshape(-1, 1, 28, 28)\n",
    "train_label = train_small.iloc[:, 0].values\n",
    "print(train_image.shape)\n",
    "print(train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_image = torch.FloatTensor(train_image)\n",
    "t_label = torch.LongTensor(train_label)\n",
    "train_dataset = list(zip(t_image, t_label))\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_image = validation_small.iloc[:, 1:].values.reshape(-1, 1, 28, 28)\n",
    "# validation_label = validation_small.iloc[:, 0].values\n",
    "# print(validation_image.shape)\n",
    "# print(validation_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_image = torch.FloatTensor(validation_image)\n",
    "# v_label = torch.LongTensor(validation_label)\n",
    "# validation_dataset = list(zip(v_image, v_label))\n",
    "# len(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = test.values.reshape(-1, 1, 28, 28)\n",
    "test_dataset = torch.FloatTensor(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset,\n",
    "#                                           batch_size=batch_size, \n",
    "#                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=len(test_dataset), \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: torch.Size([100, 1, 28, 28]) \n",
      "labels: torch.Size([100])\n",
      "tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "         0.,  0.,  0.,  0.])\n",
      "tensor(7)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print('images:', images.shape, '\\nlabels:', labels.shape)\n",
    "    print(images[1][0][6])\n",
    "    print(labels[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images: torch.Size([28000, 1, 28, 28])\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.2353,  0.8941,  0.9922,  0.9922,  0.9922,\n",
      "         0.9922,  0.9922,  0.9922,  0.9922,  0.8118,  0.7725,  0.1804,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "for images in test_loader:\n",
    "    print('images:', images.shape)\n",
    "    print(images[0][0][6])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchDeepConvNet2(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(PytorchDeepConvNet2, self).__init__()\n",
    "\n",
    "        # Layer 1: conv - relu - conv- relu - pool\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))        \n",
    "        \n",
    "        # Layer 2: conv - relu - conv- relu - pool\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # Layer 3: conv - relu - conv- relu - pool\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # Fully Connected 1 (readout)\n",
    "        # Affine - ReLU - Dropout - Affine - Dropout - Softmax\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(64 * 4 * 4, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, num_classes),\n",
    "            nn.Dropout(0.5))\n",
    "\n",
    "        # Initialize all parameters using kaiming normalization\n",
    "        self.init_weights_kaiming()\n",
    "    \n",
    "    def init_weights_kaiming(self):\n",
    "        #Use kaiming normalization to initialize the parameters\n",
    "        for layer in [self.layer1, self.layer2, self.layer3, self.layer4]:\n",
    "            for m in layer:\n",
    "                if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "                    m.weight = nn.init.kaiming_normal_(m.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "    \n",
    "        # Linear function (readout)\n",
    "        out = self.layer4(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PytorchDeepConvNet2(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=50, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=50, out_features=10, bias=True)\n",
      "    (4): Dropout(p=0.5)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = PytorchDeepConvNet2(num_classes)\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Use Adam as the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 50. Loss: 2.0896081924438477.\n",
      "Iteration: 100. Loss: 1.7662558555603027.\n",
      "Iteration: 150. Loss: 1.764256238937378.\n",
      "Iteration: 200. Loss: 1.7440763711929321.\n",
      "Iteration: 250. Loss: 1.7460557222366333.\n",
      "Iteration: 300. Loss: 1.6307404041290283.\n",
      "Iteration: 350. Loss: 1.4814486503601074.\n",
      "Iteration: 400. Loss: 1.3664772510528564.\n",
      "Iteration: 450. Loss: 1.344333291053772.\n",
      "Iteration: 500. Loss: 1.4874287843704224.\n",
      "Iteration: 550. Loss: 1.1613502502441406.\n",
      "Iteration: 600. Loss: 1.1734824180603027.\n",
      "Iteration: 650. Loss: 1.0343904495239258.\n",
      "Iteration: 700. Loss: 1.2027736902236938.\n",
      "Iteration: 750. Loss: 1.1612176895141602.\n",
      "Iteration: 800. Loss: 1.353973388671875.\n",
      "Iteration: 850. Loss: 1.2040355205535889.\n",
      "Iteration: 900. Loss: 1.2894091606140137.\n",
      "Iteration: 950. Loss: 1.3093675374984741.\n",
      "Iteration: 1000. Loss: 1.0049357414245605.\n",
      "Iteration: 1050. Loss: 1.2651605606079102.\n",
      "Iteration: 1100. Loss: 1.111684799194336.\n",
      "Iteration: 1150. Loss: 1.313420057296753.\n",
      "Iteration: 1200. Loss: 0.9948675036430359.\n",
      "Iteration: 1250. Loss: 0.9894362688064575.\n",
      "Iteration: 1300. Loss: 1.1569275856018066.\n",
      "Iteration: 1350. Loss: 1.0029274225234985.\n",
      "Iteration: 1400. Loss: 1.180722713470459.\n",
      "Iteration: 1450. Loss: 1.0575463771820068.\n",
      "Iteration: 1500. Loss: 1.0577951669692993.\n",
      "Iteration: 1550. Loss: 1.1374387741088867.\n",
      "Iteration: 1600. Loss: 0.8498163819313049.\n",
      "Iteration: 1650. Loss: 1.0474704504013062.\n",
      "Iteration: 1700. Loss: 1.1367268562316895.\n",
      "Iteration: 1750. Loss: 1.1269192695617676.\n",
      "Iteration: 1800. Loss: 1.0790022611618042.\n",
      "Iteration: 1850. Loss: 1.239148736000061.\n",
      "Iteration: 1900. Loss: 1.0703169107437134.\n",
      "Iteration: 1950. Loss: 0.9892724752426147.\n",
      "Iteration: 2000. Loss: 1.1303515434265137.\n",
      "Iteration: 2050. Loss: 0.9973430037498474.\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "# accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images) # Now we dont need to resize like images.view(xx)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients w.r.t parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate Loss: Softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Getting gradients w.r.t paramters\n",
    "        loss.backward()\n",
    "        \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        \n",
    "        if iter % 50 == 0:\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}.'.format(iter, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images in test_loader:\n",
    "        images = Variable(images)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11a7e4940>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.imshow(images[10].reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ImageId  Label\n",
       "0        1      2\n",
       "1        2      0\n",
       "2        3      9\n",
       "3        4      9\n",
       "4        5      3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame({'Label': predicted.numpy(), 'ImageId': list(range(1, predicted.shape[0] + 1))})\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('prediction.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
